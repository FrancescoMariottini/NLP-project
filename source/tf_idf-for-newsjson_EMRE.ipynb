{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Import reduce from functools\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/data/news_data.json') as f:\n",
    "      data2 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>authors</th>\n",
       "      <th>tags</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>source</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10813</td>\n",
       "      <td>ZingBox aims for ‘Internet of Trusted Things’,...</td>\n",
       "      <td>Cybersecurity provider ZingBox has announced t...</td>\n",
       "      <td>None</td>\n",
       "      <td>device\\niot\\nguardian\\napproach\\ndevices\\nindu...</td>\n",
       "      <td>Cybersecurity provider ZingBox has announced t...</td>\n",
       "      <td>https://artificialintelligence-news.com/2017/0...</td>\n",
       "      <td>AInews</td>\n",
       "      <td>2020-02-05T17:08:34.343Z</td>\n",
       "      <td>2020-02-05T17:08:34.343Z</td>\n",
       "      <td>James Bourne</td>\n",
       "      <td>2017-04-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10814</td>\n",
       "      <td>AI may help create more sustainable data centres</td>\n",
       "      <td>Enterprise data centre provider Aegis Data arg...</td>\n",
       "      <td>None</td>\n",
       "      <td>data\\ncentre\\nnatural\\nnew\\ntechnology\\nindust...</td>\n",
       "      <td>Enterprise data centre provider Aegis Data arg...</td>\n",
       "      <td>https://artificialintelligence-news.com/2017/0...</td>\n",
       "      <td>AInews</td>\n",
       "      <td>2020-02-05T17:08:34.355Z</td>\n",
       "      <td>2020-02-05T17:08:34.355Z</td>\n",
       "      <td>James Bourne</td>\n",
       "      <td>2017-04-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              title  \\\n",
       "0  10813  ZingBox aims for ‘Internet of Trusted Things’,...   \n",
       "1  10814   AI may help create more sustainable data centres   \n",
       "\n",
       "                                             summary authors  \\\n",
       "0  Cybersecurity provider ZingBox has announced t...    None   \n",
       "1  Enterprise data centre provider Aegis Data arg...    None   \n",
       "\n",
       "                                                tags  \\\n",
       "0  device\\niot\\nguardian\\napproach\\ndevices\\nindu...   \n",
       "1  data\\ncentre\\nnatural\\nnew\\ntechnology\\nindust...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Cybersecurity provider ZingBox has announced t...   \n",
       "1  Enterprise data centre provider Aegis Data arg...   \n",
       "\n",
       "                                                 url  source  \\\n",
       "0  https://artificialintelligence-news.com/2017/0...  AInews   \n",
       "1  https://artificialintelligence-news.com/2017/0...  AInews   \n",
       "\n",
       "                 created_at                updated_at        author  \\\n",
       "0  2020-02-05T17:08:34.343Z  2020-02-05T17:08:34.343Z  James Bourne   \n",
       "1  2020-02-05T17:08:34.355Z  2020-02-05T17:08:34.355Z  James Bourne   \n",
       "\n",
       "         date  \n",
       "0  2017-04-25  \n",
       "1  2017-04-25  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_cases = pd.DataFrame(data2['data'], columns=['id', 'title', 'summary', \n",
    "                                      'authors', 'tags', \n",
    "                                      'text', 'url', 'source',\n",
    "                                      'created_at', 'updated_at',\n",
    "                                      'author', 'date'])\n",
    "new_cases.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Exscientia partners with GSK to further drug discovery through AI'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_cases.title[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UK-based drug design company Exscientia has announced it has entered into a strategic drug discovery partnership with GlaxoSmithKline (GSK).\\n\\nUnder this partnership, Exscientia will use its artificial intelligence (AI)-enabled platform, combined with the expertise of GSK, to discover novel and selective small molecules for up to 10 disease-related targets, nominated by GSK across multiple therapeutic areas. It will receive research payments from GSK in lieu of this.\\n\\nExscientia will also receive near-term lead and pre-clinical candidate milestones on achieving all of the objectives. If all 10 projects are advanced, Exscientia will receive a total of £33 million from GSK on achieving these milestones. It will also receive incentives to reduce the number of compounds required for synthesis and assay in order to achieve lead and candidate compound goals.\\n\\nExscientia will use technologies like its ‘big data’ resources that comprise medicinal chemistry and large-scale bio-assays, and its AI-driven algorithms to design novel molecules that fulfil the requirements of the lead and candidate criteria.\\n\\nAndrew Hopkins, CEO of Exscientia, said: “The alliance provides further validation of our AI-driven platform and its potential to accelerate the discovery of novel, high-quality drug candidates. Applying our approach to client discovery projects has already delivered candidate-quality molecules in roughly one-quarter of the time, and at one-quarter of the cost of traditional approaches. Our intention therefore is to apply these capabilities to projects selected by GSK.\\n\\n“Delivering efficiencies to drug discovery has the potential to revolutionise the way early projects are executed, enabling more dynamic target selections from the burgeoning set of opportunities,” Hopkins added. “We look forward to a productive collaboration with GSK.”\\n\\nInterested in hearing industry leaders discuss subjects like this and sharing their use-cases? Attend the co-located AI & Big Data Expo events with upcoming shows in Silicon Valley, London and Amsterdam to learn more. Co-located with the IoT Tech Expo, Blockchain Expo and Cyber Security & Cloud Expo so you can explore the future of enterprise technology in one place.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_cases.text[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Dind topic with tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = new_cases.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning in Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import STOPWORDS\n",
    "english_stops = set(STOPWORDS)\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of texts: 1626\n",
      "Total number of tokens: 1434310\n",
      "Total of words after removing stop words: 694546\n"
     ]
    }
   ],
   "source": [
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "tokens =[word_tokenize(article) for article in texts]\n",
    "print(\"Total number of texts: {}\".format(len(tokens)))\n",
    "\n",
    "len_array = [len(token_array) for token_array in tokens]\n",
    "# Use reduce() to apply a lambda function over stark: result\n",
    "total_tokens = reduce(lambda item1, item2: item1+item2, len_array)\n",
    "print(\"Total number of tokens: {}\".format(total_tokens))\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [[t.lower() for t in token] for token in tokens]\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only_list = [[t for t in lower_token if t.isalpha()] for lower_token in lower_tokens]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [[t for t in alpha_only if t not in english_stops] for alpha_only in alpha_only_list]\n",
    "\n",
    "len_array = [len(token_array) for token_array in no_stops]\n",
    "# Use reduce() to apply a lambda function over stark: result\n",
    "total_tokens = reduce(lambda item1, item2: item1+item2, len_array)\n",
    "print(\"Total of words after removing stop words: {}\".format(total_tokens))\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "articles_lemmatized = [[wordnet_lemmatizer.lemmatize(t, get_wordnet_pos(t)) for t in no_stop] for no_stop in no_stops] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dictionary including all words in dataset (694546 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(articles_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1626\n"
     ]
    }
   ],
   "source": [
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles_lemmatized]\n",
    "\n",
    "# How much text in corpus_sorted\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Usecase with similarity method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6868798983943949\n",
      "0.6651798996139505\n",
      "0.640085929584783\n",
      "0.6884786141765988\n",
      "0.8165983805119333\n",
      "0.602837601429253\n",
      "0.7635216211151113\n",
      "0.7737061411011231\n",
      "0.443758571234073\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"Clustering, Regression, logistic, Resource allocation\")\n",
    "doc2 = nlp(\"Dimensionality reduction, Search algorithms, Predictive analytics\")\n",
    "doc3 = nlp(\"Classification, Sorting, Predictive maintenance\")\n",
    "doc4 = nlp(\"Conventional neural networks, Merging, Hyper-personalization\")\n",
    "doc5 = nlp(\"Deep learning networks, Compression, Discover new trends/anomalies\")\n",
    "doc6 = nlp(\"Convolutional neural network, Graph algorithms, Forecasting\")\n",
    "doc7 = nlp(\"Recurrent neural network Linear and non-linear optimization, Price and product optimization\")\n",
    "doc8 = nlp(\"Deep belief networks, Signal processing, Convert unstructured data\")\n",
    "doc9 = nlp(\"Encryption, Triaging\")\n",
    "docX = nlp(new_cases.text[20])\n",
    "\n",
    "# Get the similarity of doc1 and doc2\n",
    "similarity = docX.similarity(doc1)\n",
    "print(similarity)\n",
    "similarity = docX.similarity(doc2)\n",
    "print(similarity)\n",
    "similarity = docX.similarity(doc3)\n",
    "print(similarity)\n",
    "similarity = docX.similarity(doc4)\n",
    "print(similarity)\n",
    "similarity = docX.similarity(doc5)\n",
    "print(similarity)\n",
    "similarity = docX.similarity(doc6)\n",
    "print(similarity)\n",
    "similarity = docX.similarity(doc7)\n",
    "print(similarity)\n",
    "similarity = docX.similarity(doc8)\n",
    "print(similarity)\n",
    "similarity = docX.similarity(doc9)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling with LDA for all Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaMulticore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model =  LdaMulticore(corpus, \n",
    "                           num_topics = 8, \n",
    "                           id2word = dictionary,                                    \n",
    "                           passes = 10,\n",
    "                           workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.021*\"ai\" + 0.013*\"expo\" + 0.013*\"data\" + 0.012*\"will\" + 0.010*\"technology\" + 0.009*\"s\" + 0.007*\"use\" + 0.007*\"new\" + 0.006*\"learn\" + 0.006*\"industry\"\n",
      "Topic: 1 \n",
      "Words: 0.017*\"mit\" + 0.011*\"s\" + 0.010*\"say\" + 0.009*\"system\" + 0.008*\"will\" + 0.007*\"intelligence\" + 0.007*\"science\" + 0.007*\"research\" + 0.007*\"work\" + 0.007*\"computer\"\n",
      "Topic: 2 \n",
      "Words: 0.013*\"learn\" + 0.010*\"human\" + 0.009*\"say\" + 0.008*\"system\" + 0.008*\"computer\" + 0.008*\"s\" + 0.007*\"robot\" + 0.007*\"machine\" + 0.005*\"work\" + 0.005*\"data\"\n",
      "Topic: 3 \n",
      "Words: 0.018*\"s\" + 0.010*\"use\" + 0.009*\"facial\" + 0.009*\"ai\" + 0.009*\"recognition\" + 0.006*\"say\" + 0.006*\"technology\" + 0.006*\"percent\" + 0.006*\"human\" + 0.005*\"algorithm\"\n",
      "Topic: 4 \n",
      "Words: 0.014*\"s\" + 0.011*\"say\" + 0.010*\"data\" + 0.010*\"model\" + 0.009*\"use\" + 0.009*\"learn\" + 0.009*\"system\" + 0.008*\"researcher\" + 0.007*\"image\" + 0.005*\"network\"\n",
      "Topic: 5 \n",
      "Words: 0.029*\"ai\" + 0.021*\"s\" + 0.010*\"google\" + 0.009*\"company\" + 0.008*\"say\" + 0.007*\"will\" + 0.006*\"use\" + 0.006*\"technology\" + 0.006*\"make\" + 0.006*\"learn\"\n",
      "Topic: 6 \n",
      "Words: 0.014*\"say\" + 0.009*\"company\" + 0.008*\"google\" + 0.008*\"s\" + 0.007*\"machine\" + 0.006*\"learn\" + 0.006*\"game\" + 0.006*\"will\" + 0.006*\"human\" + 0.006*\"work\"\n",
      "Topic: 7 \n",
      "Words: 0.020*\"s\" + 0.011*\"say\" + 0.007*\"use\" + 0.006*\"make\" + 0.006*\"t\" + 0.005*\"people\" + 0.005*\"work\" + 0.005*\"one\" + 0.004*\"will\" + 0.004*\"video\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modelling for only 21th article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = [new_cases.title[20], new_cases.tags[20], new_cases.text[20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [word_tokenize(article) for article in parts]\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [[t.lower() for t in token] for token in tokens]\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only_list = [[t for t in lower_token if t.isalpha()] for lower_token in lower_tokens]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [[t for t in alpha_only if t not in english_stops] for alpha_only in alpha_only_list]\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "parts_lemmatized = [[wordnet_lemmatizer.lemmatize(t, get_wordnet_pos(t)) for t in no_stop] for no_stop in no_stops] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary_single = Dictionary(parts_lemmatized)\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "corpus_single = [dictionary_single.doc2bow(part) for part in parts_lemmatized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model2 =  LdaMulticore(corpus_single, \n",
    "                           num_topics = 3, \n",
    "                           id2word = dictionary,                                    \n",
    "                           passes = 10,\n",
    "                           workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.000*\"activity\" + 0.000*\"address\" + 0.000*\"united\" + 0.000*\"algorithm\" + 0.000*\"announce\" + 0.000*\"avoid\" + 0.000*\"attend\" + 0.000*\"marshall\" + 0.000*\"across\" + 0.000*\"approach\"\n",
      "Topic: 1 \n",
      "Words: 0.001*\"address\" + 0.001*\"activity\" + 0.001*\"accuracy\" + 0.001*\"across\" + 0.001*\"united\" + 0.001*\"attend\" + 0.001*\"avoid\" + 0.001*\"algorithm\" + 0.001*\"announce\" + 0.001*\"approach\"\n",
      "Topic: 2 \n",
      "Words: 0.000*\"united\" + 0.000*\"activity\" + 0.000*\"announce\" + 0.000*\"address\" + 0.000*\"across\" + 0.000*\"accuracy\" + 0.000*\"marshall\" + 0.000*\"attend\" + 0.000*\"avoid\" + 0.000*\"big\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model2.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency and Weights with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device 9\n",
      "iot 6\n",
      "solution 5\n",
      "expo 4\n",
      "medical 4\n",
      "zingbox 4\n",
      "approach 3\n",
      "data 3\n",
      "guardian 3\n",
      "new 3\n",
      "*********\n",
      "zingbox 0.5306773897793426\n",
      "device 0.2869818491447389\n",
      "guardian 0.2797327586089335\n",
      "personality 0.20979313333215804\n",
      "solution 0.16377898490507994\n",
      "behaviour 0.15129020469990984\n",
      "defend 0.15129020469990984\n",
      "medical 0.14656096355385323\n",
      "conceptualise 0.13266934744483566\n",
      "enforces 0.13266934744483566\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Save the fench revolution document: doc\n",
    "doc = corpus[0]\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Sort the doc for word frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 10 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:10]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "\n",
    "print('*********')\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 10 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:10]:\n",
    "    print(dictionary.get(term_id), weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-idf eliminate \"iot\", it supposes it insignificant because all articles are about AI and Technology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights for all texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_weights = [sorted(tfidf[doc], key=lambda w: w[1], reverse=True) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gartner 0.41867985190680573\n",
      "prentice 0.2857483516069023\n",
      "utility 0.2621374957321741\n",
      "enterprise 0.18416514544491902\n",
      "smart 0.13834487437279003\n",
      "adecco 0.13422877693712948\n",
      "tentative 0.13422877693712948\n",
      "missive 0.1233368624905319\n",
      "flexibly 0.11944936995691548\n",
      "professional 0.11766214043801765\n"
     ]
    }
   ],
   "source": [
    "# In 9th text, Sort the weights from highest to lowest: \n",
    "for term_id, weight in tfidf_weights[8][:10]:\n",
    "    print(dictionary.get(term_id), weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Name Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/becode/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/becode/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/becode/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORG ZingBox\n",
      "PERSON IoT Guardian\n",
      "ORDINAL first\n",
      "PERCENT 99.9%\n",
      "ORG Guardian\n",
      "PRODUCT IoT Guardian\n",
      "ORG Stanford University\n",
      "ORG ZingBox\n",
      "DATE zero-day\n",
      "PERSON Jerry Marshall\n",
      "ORG United Regional Health Care System\n",
      "ORG ZingBox\n",
      "PERCENT over 95%\n",
      "PERCENT about 5%\n",
      "ORG ZingBox\n",
      "LOC Silicon Valley\n",
      "GPE London\n",
      "GPE Amsterdam\n",
      "LAW the IoT Tech Expo\n",
      "LAW Blockchain Expo and Cyber Security & Cloud Expo\n",
      "CARDINAL one\n"
     ]
    }
   ],
   "source": [
    "article2 = new_cases.text[0]\n",
    "\n",
    "# Import spacy\n",
    "import spacy\n",
    "\n",
    "# Instantiate the English model: nlp\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Create a new document: doc\n",
    "doc = nlp(article2)\n",
    "\n",
    "# Print all of the found entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use NER for extracting industry from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry = ['agriculture', 'automative','consumer products', \n",
    "            'energy','finance', 'healt care','media', \n",
    "            'public and social sector','telecom', \n",
    "            'transport, travel and logistics']          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Import the PhraseMatcher and initialize it\n",
    "from spacy.matcher import Matcher\n",
    "# Import the PhraseMatcher and initialize it\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "# Load the en_core_web_md model\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "article4 = new_cases. text[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[energy]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(article4)\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(industry))\n",
    "matcher.add('INDUSTRY', None, *patterns)\n",
    "\n",
    "# Call the matcher on the test document and print the result\n",
    "matches_single = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matches_single])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldocs = [nlp(article) for article in new_cases.text]\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(industry))\n",
    "matcher.add('INDUSTRY', None, *patterns)\n",
    "\n",
    "# Call the matcher on the test document and print the result\n",
    "matches = [matcher(doc) for doc in alldocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_word = [[alldocs[i][start:end] for match_id, start, end in match] for i, match in enumerate(matches)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_empty=0\n",
    "\n",
    "for match in matches_word:\n",
    "    if len(match)==0:\n",
    "        number_empty+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1626, 1317)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alldocs), number_empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
