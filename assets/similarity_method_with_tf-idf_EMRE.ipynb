{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import reduce from functools\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_words = {\n",
    "    'automative': ['automotive', 'taxi', 'wheel', 'fuel', 'car','drive','auto','selfdrive','vehicle','road','automobile'],\n",
    "    'Manufacturing': ['cleantech', 'deindustrialization', 'prefabrication', 'manufacturing', 'vitrification', 'fabrication' 'R&D','quality','produce','goods','factory','equipment'],\n",
    "    'Consumer Products' : ['product', 'price', 'goods', 'commerce', 'economic', 'customer','marketing','demand','inventory','supply'],\n",
    "    'Finance' : ['bank', 'money', 'capitalization', 'interest', 'fund', 'finance', 'asset','risk','loan','credit','fraud'],\n",
    "    'Agriculture' :['soil', 'grain', 'agriculture', 'field','farm','soil','weather','crop','grow','animal', 'food' , 'land'],\n",
    "    'Energy' : ['renewable', 'sustainable', 'green', 'electricity', 'energy', 'power','mines','solar','light','metal','electric','carbon', 'electonic','wind','speed'],\n",
    "    'Health Care' : ['Health', 'Care', 'emergency','doctor','wellness','patient','hospital', 'clinic','treatment','disease', 'medical','cancer'],\n",
    "    'Pharmaceuticals' : ['dose', 'pillbox', 'tonic', 'tablet', 'placebo', 'medicate', 'hospital', 'Pharmaceutical', 'drug','diagnose', 'test','trial','medicine', 'vaccine'],\n",
    "    'Public and Social sector' : ['social', 'law','crime','terrorism','policing','govern', 'public', 'infrastructure', 'education', 'tax', 'urban', 'life', 'job','enforcement', 'surveillance'],\n",
    "    'Media' :['mainstream', 'publishing', 'medium', 'social','media','video','content','news','release','film', 'press', 'viral', 'game'],\n",
    "    'Telecom' : ['location', 'station', 'host', 'telecom', 'mobile', 'voice','call','subscription','network','phone', 'broadcast', 'internet','communication' ,'modulation'],\n",
    "    'Transport & Logistics' : ['transport' , 'logistic', 'mail','parcel','travel','route','planes','truck', 'shipping', 'mobility', 'movement']\n",
    "}\n",
    "for keys, value in industry_words.items():\n",
    "    industry_words[keys] = \" \".join(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('news_data.json') as f:\n",
    "      data2 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      id                                              title  \\\n",
       "0  10813  ZingBox aims for ‘Internet of Trusted Things’,...   \n",
       "1  10814   AI may help create more sustainable data centres   \n",
       "\n",
       "                                             summary authors  \\\n",
       "0  Cybersecurity provider ZingBox has announced t...    None   \n",
       "1  Enterprise data centre provider Aegis Data arg...    None   \n",
       "\n",
       "                                                tags  \\\n",
       "0  device\\niot\\nguardian\\napproach\\ndevices\\nindu...   \n",
       "1  data\\ncentre\\nnatural\\nnew\\ntechnology\\nindust...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Cybersecurity provider ZingBox has announced t...   \n",
       "1  Enterprise data centre provider Aegis Data arg...   \n",
       "\n",
       "                                                 url  source  \\\n",
       "0  https://artificialintelligence-news.com/2017/0...  AInews   \n",
       "1  https://artificialintelligence-news.com/2017/0...  AInews   \n",
       "\n",
       "                 created_at                updated_at        author  \\\n",
       "0  2020-02-05T17:08:34.343Z  2020-02-05T17:08:34.343Z  James Bourne   \n",
       "1  2020-02-05T17:08:34.355Z  2020-02-05T17:08:34.355Z  James Bourne   \n",
       "\n",
       "         date  \n",
       "0  2017-04-25  \n",
       "1  2017-04-25  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>summary</th>\n      <th>authors</th>\n      <th>tags</th>\n      <th>text</th>\n      <th>url</th>\n      <th>source</th>\n      <th>created_at</th>\n      <th>updated_at</th>\n      <th>author</th>\n      <th>date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10813</td>\n      <td>ZingBox aims for ‘Internet of Trusted Things’,...</td>\n      <td>Cybersecurity provider ZingBox has announced t...</td>\n      <td>None</td>\n      <td>device\\niot\\nguardian\\napproach\\ndevices\\nindu...</td>\n      <td>Cybersecurity provider ZingBox has announced t...</td>\n      <td>https://artificialintelligence-news.com/2017/0...</td>\n      <td>AInews</td>\n      <td>2020-02-05T17:08:34.343Z</td>\n      <td>2020-02-05T17:08:34.343Z</td>\n      <td>James Bourne</td>\n      <td>2017-04-25</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10814</td>\n      <td>AI may help create more sustainable data centres</td>\n      <td>Enterprise data centre provider Aegis Data arg...</td>\n      <td>None</td>\n      <td>data\\ncentre\\nnatural\\nnew\\ntechnology\\nindust...</td>\n      <td>Enterprise data centre provider Aegis Data arg...</td>\n      <td>https://artificialintelligence-news.com/2017/0...</td>\n      <td>AInews</td>\n      <td>2020-02-05T17:08:34.355Z</td>\n      <td>2020-02-05T17:08:34.355Z</td>\n      <td>James Bourne</td>\n      <td>2017-04-25</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "new_cases = pd.DataFrame(data2['data'], columns=['id', 'title', 'summary', \n",
    "                                      'authors', 'tags', \n",
    "                                      'text', 'url', 'source',\n",
    "                                      'created_at', 'updated_at',\n",
    "                                      'author', 'date'])\n",
    "new_cases.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       id                                              title  \\\n",
       "16  10829  Here’s how AI can assist medical science in te...   \n",
       "\n",
       "                                              summary authors  \\\n",
       "16  Artificial intelligence (AI) and deep learning...    None   \n",
       "\n",
       "                           tags  \\\n",
       "16  diseases\\nmedical\\nai\\nexpo   \n",
       "\n",
       "                                                 text  \\\n",
       "16  Artificial intelligence (AI) and deep learning...   \n",
       "\n",
       "                                                  url  source  \\\n",
       "16  https://artificialintelligence-news.com/2017/0...  AInews   \n",
       "\n",
       "                  created_at                updated_at        author  \\\n",
       "16  2020-02-05T17:08:34.515Z  2020-02-05T17:08:34.515Z  James Bourne   \n",
       "\n",
       "          date  \n",
       "16  2017-06-12  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>summary</th>\n      <th>authors</th>\n      <th>tags</th>\n      <th>text</th>\n      <th>url</th>\n      <th>source</th>\n      <th>created_at</th>\n      <th>updated_at</th>\n      <th>author</th>\n      <th>date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>16</th>\n      <td>10829</td>\n      <td>Here’s how AI can assist medical science in te...</td>\n      <td>Artificial intelligence (AI) and deep learning...</td>\n      <td>None</td>\n      <td>diseases\\nmedical\\nai\\nexpo</td>\n      <td>Artificial intelligence (AI) and deep learning...</td>\n      <td>https://artificialintelligence-news.com/2017/0...</td>\n      <td>AInews</td>\n      <td>2020-02-05T17:08:34.515Z</td>\n      <td>2020-02-05T17:08:34.515Z</td>\n      <td>James Bourne</td>\n      <td>2017-06-12</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "new_cases[new_cases.id==10829]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spacy\n",
    "import spacy\n",
    "\n",
    "# Instantiate the English model: nlp\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = new_cases.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import STOPWORDS\n",
    "english_stops = set(STOPWORDS)\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Lower all words and Remove non-alpha, stopwords, no-noun or no-verb and Lemmatize all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total number of texts: 1626\n",
      "Total number of tokens: 1434310\n",
      "Total of words after removing stop words: 694546\n",
      "Total of words after removing words except noun: 571569\n"
     ]
    }
   ],
   "source": [
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "tokens =[word_tokenize(article) for article in texts]\n",
    "print(\"Total number of texts: {}\".format(len(tokens)))\n",
    "\n",
    "len_array = [len(token_array) for token_array in tokens]\n",
    "# Use reduce() to apply a lambda function over stark: result\n",
    "total_tokens = reduce(lambda item1, item2: item1+item2, len_array)\n",
    "print(\"Total number of tokens: {}\".format(total_tokens))\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [[t.lower() for t in token] for token in tokens]\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only_list = [[t for t in lower_token if t.isalpha()] for lower_token in lower_tokens]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [[t for t in alpha_only if t not in english_stops] for alpha_only in alpha_only_list]\n",
    "\n",
    "len_array = [len(token_array) for token_array in no_stops]\n",
    "# Use reduce() to apply a lambda function over stark: result\n",
    "total_tokens = reduce(lambda item1, item2: item1+item2, len_array)\n",
    "print(\"Total of words after removing stop words: {}\".format(total_tokens))\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "articles_lemmatized = [[wordnet_lemmatizer.lemmatize(t, get_wordnet_pos(t)) for t in no_stop if nltk.pos_tag([t])[0][1][0].upper()=='N' or nltk.pos_tag([t])[0][1][0].upper()=='V'] for no_stop in no_stops]\n",
    "\n",
    "len_array = [len(article) for article in articles_lemmatized]\n",
    "# Use reduce() to apply a lambda function over stark: result\n",
    "total_tokens = reduce(lambda item1, item2: item1+item2, len_array)\n",
    "print(\"Total of words after removing words except noun: {}\".format(total_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Remove organization, location nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(new_cases.shape[0]):\n",
    "    # Create a new document: doc\n",
    "    doc = nlp(new_cases.text[i])\n",
    "\n",
    "    # Print all of the found entities and their labels\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_=='ORG' or ent.label_=='GPE' or ent.label_=='LOC':\n",
    "            words = ent.text.split()\n",
    "            for word in words:\n",
    "                word = word.lower()\n",
    "                while word in articles_lemmatized[i]:\n",
    "                    articles_lemmatized[i].remove(word) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total of words after removing words in ORG, GPE, LOC: 503028\n"
     ]
    }
   ],
   "source": [
    "len_array = [len(article) for article in articles_lemmatized]\n",
    "# Use reduce() to apply a lambda function over stark: result\n",
    "total_tokens = reduce(lambda item1, item2: item1+item2, len_array)\n",
    "print(\"Total of words after removing words in ORG, GPE, LOC: {}\".format(total_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Predict industry type with cleaned texts by using similarity method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert list into string for each text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_string = [\" \".join(article) for article in articles_lemmatized]\n",
    "key_list = list(industry_words.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create nlp object for each industry category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_industry_list = []\n",
    "for key, value in industry_words.items():\n",
    "        doc_industry_list.append(nlp(industry_words[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example : 16th text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s how AI can assist medical science in telling a patient’s lifespan\n",
      "0.5083207819127182\n",
      "0.5599038223425676\n",
      "0.6823005964751405\n",
      "0.6627079356726187\n",
      "0.6193198169612352\n",
      "0.5920631884483465\n",
      "0.7159515352149979\n",
      "0.6621734547359772\n",
      "0.7709572952570025\n",
      "0.7063107083724028\n",
      "0.6536469096643308\n",
      "0.6471683954877313\n",
      "Public and Social sector\n"
     ]
    }
   ],
   "source": [
    "# Create nlp object for an cleaned article\n",
    "doc_article = nlp(articles_string[16])\n",
    "print(new_cases.title[16])\n",
    "similarities = []\n",
    "for doc_industry in doc_industry_list:\n",
    "    similarity = doc_article.similarity(doc_industry)\n",
    "    print(similarity)\n",
    "    similarities.append(similarity)\n",
    "max_value = max(similarities)\n",
    "max_position = similarities.index(max_value)\n",
    "industry_type = key_list[max_position]\n",
    "print(industry_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict industry type with similarity method along two nlp object for all texts in industry_type_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_type_list = []\n",
    "for article in articles_string:\n",
    "    # Create nlp object for an cleaned article\n",
    "    doc_article = nlp(article)\n",
    "    similarities = []\n",
    "    for doc_industry in doc_industry_list:\n",
    "        similarities.append(doc_article.similarity(doc_industry))\n",
    "    max_value = max(similarities)\n",
    "    max_position = similarities.index(max_value)\n",
    "    industry_type = key_list[max_position]\n",
    "    industry_type_list.append(industry_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Predict industry type with tf-idf words by using similarity method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1626\n"
     ]
    }
   ],
   "source": [
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(articles_lemmatized)\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles_lemmatized]\n",
    "\n",
    "# How much text in corpus_sorted\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tf-idf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "tfidf_weights = [sorted(tfidf[doc], key=lambda w: w[1], reverse=True) for doc in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create nlp object for each industry category and key_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = list(industry_words.keys())\n",
    "doc_industry_list = []\n",
    "for key, value in industry_words.items():\n",
    "        doc_industry_list.append(nlp(industry_words[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example : 16th text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'lifespan poll adelaide surpasses disease prediction ai patient congestive emphysema survey ascertain job absence chronic organ image report resident commission'"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "article_string_single = [dictionary.get(term_id) for term_id, weight in tfidf_weights[16][:20]]\n",
    "article_string_single = \" \".join(article_string_single)\n",
    "article_string_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.40368552190838985\n0.4599219946234982\n0.536596037573089\n0.5736589998630274\n0.549153651399558\n0.47661140152858783\n0.7848336852301346\n0.7033189258214428\n0.6473614201741285\n0.5610149268507558\n0.5194483004976785\n0.5220977663151091\nHealth Care\nPharmaceuticals\n"
     ]
    }
   ],
   "source": [
    "doc_article = nlp(article_string_single)\n",
    "similarities = []\n",
    "for doc_industry in doc_industry_list:\n",
    "    similarity = doc_article.similarity(doc_industry)\n",
    "    print(similarity)\n",
    "    similarities.append(similarity)\n",
    "max_value = max(similarities)\n",
    "industry1_pos = similarities.index(max_value)\n",
    "industry_type1 = key_list[industry1_pos]\n",
    "similarities[industry1_pos] = 0\n",
    "second_max = max(similarities)\n",
    "industry2_pos = similarities.index(second_max)\n",
    "industry_type2 = key_list[industry2_pos]\n",
    "\n",
    "print(industry_type1)\n",
    "print(industry_type2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict industry type with similarity method along two nlp object for all texts in industry_type_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select the most important first 20 words for each text a and convert list into string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_strings = [[dictionary.get(term_id) for term_id, weight in tfidf_weight[:20]] for tfidf_weight in tfidf_weights]\n",
    "articles_strings = [\" \".join(articles_string) for articles_string in articles_strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'lifespan poll adelaide surpasses disease prediction ai patient congestive emphysema survey ascertain job absence chronic organ image report resident commission'"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "articles_strings[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_type1_list = []\n",
    "industry_type2_list = []\n",
    "\n",
    "for article in articles_strings:\n",
    "    # Create nlp object for an cleaned article\n",
    "    doc_article = nlp(article)\n",
    "    similarities = []\n",
    "    for doc_industry in doc_industry_list:\n",
    "        similarities.append(doc_article.similarity(doc_industry))\n",
    "    max_value = max(similarities)\n",
    "    max_position = similarities.index(max_value)\n",
    "    industry_type = key_list[max_position]\n",
    "    industry_type_list.append(industry_type)\n",
    "\n",
    "    max_value = max(similarities)\n",
    "    industry1_pos = similarities.index(max_value)\n",
    "    industry_type1 = key_list[industry1_pos]\n",
    "    industry_type1_list.append(industry_type1)\n",
    "    similarities[industry1_pos] = 0\n",
    "    second_max = max(similarities)\n",
    "    industry2_pos = similarities.index(second_max)\n",
    "    industry_type2 = key_list[industry2_pos]\n",
    "    industry_type2_list.append(industry_type2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Public and Social sector',\n",
       " 'Consumer Products',\n",
       " 'Media',\n",
       " 'Public and Social sector',\n",
       " 'Consumer Products']"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "industry_type1_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Consumer Products', 'Energy', 'Telecom', 'Finance', 'Telecom']"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "industry_type2_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = new_cases[['text']]\n",
    "df['Industry1'] = industry_type1_list\n",
    "df['Industry2'] = industry_type2_list\n",
    "#df.assign(column_name = industry_type1_list)\n",
    "#new_cases.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv ('df.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dfd63af6506f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'new_file.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.to_csv('new_file.csv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['text']\n",
    "y = df.drop('text',axis=1)\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X,y, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([['Media', 'Telecom'],\n",
       "       ['Transport & Logistics', 'automative'],\n",
       "       ['Media', 'Consumer Products'],\n",
       "       ...,\n",
       "       ['Telecom', 'Media'],\n",
       "       ['Telecom', 'Energy'],\n",
       "       ['Public and Social sector', 'Media']], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "source": [
    "ytrain[['Industry1','Industry2']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform(ytrain[['Industry1','Industry2']].values)\n",
    "test_labels = mlb.fit_transform(ytest[['Industry1','Industry2']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['Agriculture', 'Consumer Products', 'Energy', 'Finance',\n",
       "       'Health Care', 'Manufacturing', 'Media', 'Pharmaceuticals',\n",
       "       'Public and Social sector', 'Telecom', 'Transport & Logistics',\n",
       "       'automative'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "source": [
    "mlb.classes_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Spacy model, we ignore \"tagger\", \"parser\", \"ner\" to make processing faster\n",
    "nlp = en_core_web_md.load(disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "\n",
    "def keep_token(t):\n",
    "    # remove stop words, punct, numbers\n",
    "    return (t.is_alpha and\n",
    "            not (t.is_space or t.is_punct or\n",
    "                 t.is_stop or t.like_num))\n",
    "\n",
    "def lemmatize_doc(doc):\n",
    "    # Lemmatize\n",
    "    return [ t.lemma_ for t in doc if keep_token(t)]\n",
    "\n",
    "# apply on text filed to get clean_text\n",
    "df['clean_text'] = df.text.apply(lambda x: ' '.join(lemmatize_doc(nlp(x.lower()))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import STOPWORDS\n",
    "english_stops = set(STOPWORDS)\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def tokenize_lemma_stopwords(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    tokens = nltk.tokenize.word_tokenize(text.lower()) # split string into words (tokens)\n",
    "    tokens = [t for t in tokens if t.isalpha()] # keep strings with only alphabets\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] # put words into base form\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "    tokens = [t for t in tokens if len(t) > 2] # remove short words, they're probably not useful\n",
    "    tokens = [t for t in tokens if t not in english_stops] # remove stopwords\n",
    "    cleanedText = \" \".join(tokens)\n",
    "    return cleanedText\n",
    "\n",
    "def dataCleaning(df):\n",
    "    data = df.copy()\n",
    "    data = data.apply(tokenize_lemma_stopwords)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedTrainData = dataCleaning(xtrain)\n",
    "cleanedTestData = dataCleaning(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorised_train_documents = vectorizer.fit_transform(cleanedTrainData)\n",
    "vectorised_test_documents = vectorizer.transform(cleanedTestData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, hamming_loss\n",
    "\n",
    "ModelsPerformance = {}\n",
    "\n",
    "def metricsReport(modelName, test_labels, predictions):\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "\n",
    "    macro_precision = precision_score(test_labels, predictions, average='macro')\n",
    "    macro_recall = recall_score(test_labels, predictions, average='macro')\n",
    "    macro_f1 = f1_score(test_labels, predictions, average='macro')\n",
    "\n",
    "    micro_precision = precision_score(test_labels, predictions, average='micro')\n",
    "    micro_recall = recall_score(test_labels, predictions, average='micro')\n",
    "    micro_f1 = f1_score(test_labels, predictions, average='micro')\n",
    "    hamLoss = hamming_loss(test_labels, predictions)\n",
    "    print(\"------\" + modelName + \" Model Metrics-----\")\n",
    "    print(\"Accuracy: {:.4f}\\nHamming Loss: {:.4f}\\nPrecision:\\n  - Macro: {:.4f}\\n  - Micro: {:.4f}\\nRecall:\\n  - Macro: {:.4f}\\n  - Micro: {:.4f}\\nF1-measure:\\n  - Macro: {:.4f}\\n  - Micro: {:.4f}\"\\\n",
    "          .format(accuracy, hamLoss, macro_precision, micro_precision, macro_recall, micro_recall, macro_f1, micro_f1))\n",
    "    ModelsPerformance[modelName] = micro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1300, 18051)"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "vectorised_train_documents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2, 10)"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------knn Model Metrics-----\nAccuracy: 0.3405\nHamming Loss: 0.1061\nPrecision:\n  - Macro: 0.7435\n  - Micro: 0.7319\nRecall:\n  - Macro: 0.4496\n  - Micro: 0.5736\nF1-measure:\n  - Macro: 0.5304\n  - Micro: 0.6432\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "knnClf = KNeighborsClassifier()\n",
    "\n",
    "knnClf.fit(vectorised_train_documents, train_labels)\n",
    "knnPredictions = knnClf.predict(vectorised_test_documents)\n",
    "metricsReport(\"knn\", test_labels, knnPredictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------Decision Tree Model Metrics-----\nAccuracy: 0.2025\nHamming Loss: 0.1759\nPrecision:\n  - Macro: 0.3748\n  - Micro: 0.4724\nRecall:\n  - Macro: 0.3798\n  - Micro: 0.4724\nF1-measure:\n  - Macro: 0.3718\n  - Micro: 0.4724\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtClassifier = DecisionTreeClassifier()\n",
    "dtClassifier.fit(vectorised_train_documents, train_labels)\n",
    "dtPreds = dtClassifier.predict(vectorised_test_documents)\n",
    "metricsReport(\"Decision Tree\", test_labels, dtPreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------Bagging Model Metrics-----\nAccuracy: 0.2025\nHamming Loss: 0.1181\nPrecision:\n  - Macro: 0.6355\n  - Micro: 0.7262\nRecall:\n  - Macro: 0.3913\n  - Micro: 0.4678\nF1-measure:\n  - Macro: 0.4704\n  - Micro: 0.5690\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bagClassifier = OneVsRestClassifier(BaggingClassifier(n_jobs=-1))\n",
    "bagClassifier.fit(vectorised_train_documents, train_labels)\n",
    "bagPreds = bagClassifier.predict(vectorised_test_documents)\n",
    "metricsReport(\"Bagging\", test_labels, bagPreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------Random Forest Model Metrics-----\nAccuracy: 0.0828\nHamming Loss: 0.1286\nPrecision:\n  - Macro: 0.5381\n  - Micro: 0.7560\nRecall:\n  - Macro: 0.1475\n  - Micro: 0.3374\nF1-measure:\n  - Macro: 0.1798\n  - Micro: 0.4666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfClassifier = RandomForestClassifier(n_jobs=-1)\n",
    "rfClassifier.fit(vectorised_train_documents, train_labels)\n",
    "rfPreds = rfClassifier.predict(vectorised_test_documents)\n",
    "metricsReport(\"Random Forest\", test_labels, rfPreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------Boosting Model Metrics-----\nAccuracy: 0.2209\nHamming Loss: 0.1138\nPrecision:\n  - Macro: 0.6769\n  - Micro: 0.7390\nRecall:\n  - Macro: 0.3962\n  - Micro: 0.4908\nF1-measure:\n  - Macro: 0.4816\n  - Micro: 0.5899\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "boostClassifier = OneVsRestClassifier(GradientBoostingClassifier())\n",
    "boostClassifier.fit(vectorised_train_documents, train_labels)\n",
    "boostPreds = boostClassifier.predict(vectorised_test_documents)\n",
    "metricsReport(\"Boosting\", test_labels, boostPreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------Multinomial NB Model Metrics-----\nAccuracy: 0.0245\nHamming Loss: 0.1309\nPrecision:\n  - Macro: 0.1571\n  - Micro: 0.8977\nRecall:\n  - Macro: 0.0788\n  - Micro: 0.2423\nF1-measure:\n  - Macro: 0.0922\n  - Micro: 0.3816\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nbClassifier = OneVsRestClassifier(MultinomialNB())\n",
    "nbClassifier.fit(vectorised_train_documents, train_labels)\n",
    "\n",
    "nbPreds = nbClassifier.predict(vectorised_test_documents)\n",
    "metricsReport(\"Multinomial NB\", test_labels, nbPreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------SVC Sq. Hinge Loss Model Metrics-----\nAccuracy: 0.3190\nHamming Loss: 0.0920\nPrecision:\n  - Macro: 0.8128\n  - Micro: 0.8120\nRecall:\n  - Macro: 0.4481\n  - Micro: 0.5828\nF1-measure:\n  - Macro: 0.5575\n  - Micro: 0.6786\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svmClassifier = OneVsRestClassifier(LinearSVC(), n_jobs=-1)\n",
    "svmClassifier.fit(vectorised_train_documents, train_labels)\n",
    "\n",
    "svmPreds = svmClassifier.predict(vectorised_test_documents)\n",
    "metricsReport(\"SVC Sq. Hinge Loss\", test_labels, svmPreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------Power Set SVC Model Metrics-----\nAccuracy: 0.4724\nHamming Loss: 0.1048\nPrecision:\n  - Macro: 0.7078\n  - Micro: 0.6856\nRecall:\n  - Macro: 0.5795\n  - Micro: 0.6856\nF1-measure:\n  - Macro: 0.6118\n  - Micro: 0.6856\n"
     ]
    }
   ],
   "source": [
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "\n",
    "powerSetSVC = LabelPowerset(LinearSVC())\n",
    "powerSetSVC.fit(vectorised_train_documents, train_labels)\n",
    "\n",
    "powerSetSVCPreds = powerSetSVC.predict(vectorised_test_documents)\n",
    "metricsReport(\"Power Set SVC\", test_labels, powerSetSVCPreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Model Name           | Micro-F1 Score\n-------------------------------------------\n  knn                  | 0.643164230438521\n-------------------------------------------\n  Decision Tree        | 0.4723926380368098\n-------------------------------------------\n  Bagging              | 0.5690298507462687\n-------------------------------------------\n  Multinomial NB       | 0.3816425120772947\n-------------------------------------------\n  SVC Sq. Hinge Loss   | 0.6785714285714286\n-------------------------------------------\n  Boosting             | 0.5898617511520737\n-------------------------------------------\n  Random Forest        | 0.4665959703075292\n-------------------------------------------\n  Power Set SVC        | 0.6855828220858896\n-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Comparison on different models based on their Micro-F1 score\n",
    "\n",
    "\n",
    "print(\"  Model Name \" + \" \"*10 + \"| Micro-F1 Score\")\n",
    "print(\"-------------------------------------------\")\n",
    "for key, value in ModelsPerformance.items():\n",
    "    print(\"  \" + key, \" \"*(20-len(key)) + \"|\", value)\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get the model\n",
    "def get_model(n_inputs, n_outputs):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(20, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
    "\tmodel.add(Dense(n_outputs, activation='sigmoid'))\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make a prediction on the test set\n",
    "yhat = model.predict(X_test)\n",
    "# round probabilities to class labels\n",
    "yhat = yhat.round()\n",
    "# calculate accuracy\n",
    "acc = accuracy_score(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X, y):\n",
    "\tresults = list()\n",
    "\tn_inputs, n_outputs = X.shape[1], y.shape[1]\n",
    "\t# define evaluation procedure\n",
    "\tcv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\t# enumerate folds\n",
    "\tfor train_ix, test_ix in cv.split(X):\n",
    "\t\t# prepare data\n",
    "\t\tX_train, X_test = X[train_ix], X[test_ix]\n",
    "\t\ty_train, y_test = y[train_ix], y[test_ix]\n",
    "\t\t# define model\n",
    "\t\tmodel = get_model(n_inputs, n_outputs)\n",
    "\t\t# fit model\n",
    "\t\tmodel.fit(X_train, y_train, verbose=0, epochs=100)\n",
    "\t\t# make a prediction on the test set\n",
    "\t\tyhat = model.predict(X_test)\n",
    "\t\t# round probabilities to class labels\n",
    "\t\tyhat = yhat.round()\n",
    "\t\t# calculate accuracy\n",
    "\t\tacc = accuracy_score(y_test, yhat)\n",
    "\t\t# store result\n",
    "\t\tprint('>%.3f' % acc)\n",
    "\t\tresults.append(acc)\n",
    "\treturn results\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_model(X, y)\n",
    "# summarize performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(results), std(results)))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}