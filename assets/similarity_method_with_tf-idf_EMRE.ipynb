{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import reduce from functools\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_words = {\n",
    "    'automative': ['automotive', 'taxi', 'wheel', 'fuel', 'car','drive','auto','selfdrive','vehicle','road','automobile'],\n",
    "    'Manufacturing': ['cleantech', 'deindustrialization', 'prefabrication', 'manufacturing', 'vitrification', 'fabrication' 'R&D','quality','produce','goods','factory','equipment'],\n",
    "    'Consumer Products' : ['product', 'price', 'goods', 'commerce', 'economic', 'customer','marketing','demand','inventory','supply'],\n",
    "    'Finance' : ['bank', 'money', 'capitalization', 'interest', 'fund', 'finance', 'asset','risk','loan','credit','fraud'],\n",
    "    'Agriculture' :['soil', 'grain', 'agriculture', 'field','farm','soil','weather','crop','grow','animal', 'food' , 'land'],\n",
    "    'Energy' : ['renewable', 'sustainable', 'green', 'electricity', 'energy', 'power','mines','solar','light','metal','electric','carbon', 'electonic','wind','speed'],\n",
    "    'Health Care' : ['Health', 'Care', 'emergency','doctor','wellness','patient','hospital', 'clinic','treatment','disease', 'medical','cancer'],\n",
    "    'Pharmaceuticals' : ['dose', 'pillbox', 'tonic', 'tablet', 'placebo', 'medicate', 'hospital', 'Pharmaceutical', 'drug','diagnose', 'test','trial','medicine', 'vaccine'],\n",
    "    'Public and Social sector' : ['social', 'law','crime','terrorism','policing','govern', 'public', 'infrastructure', 'education', 'tax', 'urban', 'life', 'job','enforcement', 'surveillance'],\n",
    "    'Media' :['mainstream', 'publishing', 'medium', 'social','media','video','content','news','release','film', 'press', 'viral', 'game'],\n",
    "    'Telecom' : ['location', 'station', 'host', 'telecom', 'mobile', 'voice','call','subscription','network','phone', 'broadcast', 'internet','communication' ,'modulation'],\n",
    "    'Transport & Logistics' : ['transport' , 'logistic', 'mail','parcel','travel','route','planes','truck', 'shipping', 'mobility', 'movement']\n",
    "}\n",
    "for keys, value in industry_words.items():\n",
    "    industry_words[keys] = \" \".join(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/data/news_data.json') as f:\n",
    "      data2 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>authors</th>\n",
       "      <th>tags</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>source</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10813</td>\n",
       "      <td>ZingBox aims for ‘Internet of Trusted Things’,...</td>\n",
       "      <td>Cybersecurity provider ZingBox has announced t...</td>\n",
       "      <td>None</td>\n",
       "      <td>device\\niot\\nguardian\\napproach\\ndevices\\nindu...</td>\n",
       "      <td>Cybersecurity provider ZingBox has announced t...</td>\n",
       "      <td>https://artificialintelligence-news.com/2017/0...</td>\n",
       "      <td>AInews</td>\n",
       "      <td>2020-02-05T17:08:34.343Z</td>\n",
       "      <td>2020-02-05T17:08:34.343Z</td>\n",
       "      <td>James Bourne</td>\n",
       "      <td>2017-04-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10814</td>\n",
       "      <td>AI may help create more sustainable data centres</td>\n",
       "      <td>Enterprise data centre provider Aegis Data arg...</td>\n",
       "      <td>None</td>\n",
       "      <td>data\\ncentre\\nnatural\\nnew\\ntechnology\\nindust...</td>\n",
       "      <td>Enterprise data centre provider Aegis Data arg...</td>\n",
       "      <td>https://artificialintelligence-news.com/2017/0...</td>\n",
       "      <td>AInews</td>\n",
       "      <td>2020-02-05T17:08:34.355Z</td>\n",
       "      <td>2020-02-05T17:08:34.355Z</td>\n",
       "      <td>James Bourne</td>\n",
       "      <td>2017-04-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              title  \\\n",
       "0  10813  ZingBox aims for ‘Internet of Trusted Things’,...   \n",
       "1  10814   AI may help create more sustainable data centres   \n",
       "\n",
       "                                             summary authors  \\\n",
       "0  Cybersecurity provider ZingBox has announced t...    None   \n",
       "1  Enterprise data centre provider Aegis Data arg...    None   \n",
       "\n",
       "                                                tags  \\\n",
       "0  device\\niot\\nguardian\\napproach\\ndevices\\nindu...   \n",
       "1  data\\ncentre\\nnatural\\nnew\\ntechnology\\nindust...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Cybersecurity provider ZingBox has announced t...   \n",
       "1  Enterprise data centre provider Aegis Data arg...   \n",
       "\n",
       "                                                 url  source  \\\n",
       "0  https://artificialintelligence-news.com/2017/0...  AInews   \n",
       "1  https://artificialintelligence-news.com/2017/0...  AInews   \n",
       "\n",
       "                 created_at                updated_at        author  \\\n",
       "0  2020-02-05T17:08:34.343Z  2020-02-05T17:08:34.343Z  James Bourne   \n",
       "1  2020-02-05T17:08:34.355Z  2020-02-05T17:08:34.355Z  James Bourne   \n",
       "\n",
       "         date  \n",
       "0  2017-04-25  \n",
       "1  2017-04-25  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_cases = pd.DataFrame(data2['data'], columns=['id', 'title', 'summary', \n",
    "                                      'authors', 'tags', \n",
    "                                      'text', 'url', 'source',\n",
    "                                      'created_at', 'updated_at',\n",
    "                                      'author', 'date'])\n",
    "new_cases.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>authors</th>\n",
       "      <th>tags</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>source</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10829</td>\n",
       "      <td>Here’s how AI can assist medical science in te...</td>\n",
       "      <td>Artificial intelligence (AI) and deep learning...</td>\n",
       "      <td>None</td>\n",
       "      <td>diseases\\nmedical\\nai\\nexpo</td>\n",
       "      <td>Artificial intelligence (AI) and deep learning...</td>\n",
       "      <td>https://artificialintelligence-news.com/2017/0...</td>\n",
       "      <td>AInews</td>\n",
       "      <td>2020-02-05T17:08:34.515Z</td>\n",
       "      <td>2020-02-05T17:08:34.515Z</td>\n",
       "      <td>James Bourne</td>\n",
       "      <td>2017-06-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                              title  \\\n",
       "16  10829  Here’s how AI can assist medical science in te...   \n",
       "\n",
       "                                              summary authors  \\\n",
       "16  Artificial intelligence (AI) and deep learning...    None   \n",
       "\n",
       "                           tags  \\\n",
       "16  diseases\\nmedical\\nai\\nexpo   \n",
       "\n",
       "                                                 text  \\\n",
       "16  Artificial intelligence (AI) and deep learning...   \n",
       "\n",
       "                                                  url  source  \\\n",
       "16  https://artificialintelligence-news.com/2017/0...  AInews   \n",
       "\n",
       "                  created_at                updated_at        author  \\\n",
       "16  2020-02-05T17:08:34.515Z  2020-02-05T17:08:34.515Z  James Bourne   \n",
       "\n",
       "          date  \n",
       "16  2017-06-12  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_cases[new_cases.id==10829]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spacy\n",
    "import spacy\n",
    "\n",
    "# Instantiate the English model: nlp\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = new_cases.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import STOPWORDS\n",
    "english_stops = set(STOPWORDS)\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Lower all words and Remove non-alpha, stopwords, no-noun or no-verb and Lemmatize all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of texts: 1626\n",
      "Total number of tokens: 1434310\n",
      "Total of words after removing stop words: 694546\n",
      "Total of words after removing words except noun: 478185\n"
     ]
    }
   ],
   "source": [
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "tokens =[word_tokenize(article) for article in texts]\n",
    "print(\"Total number of texts: {}\".format(len(tokens)))\n",
    "\n",
    "len_array = [len(token_array) for token_array in tokens]\n",
    "# Use reduce() to apply a lambda function over stark: result\n",
    "total_tokens = reduce(lambda item1, item2: item1+item2, len_array)\n",
    "print(\"Total number of tokens: {}\".format(total_tokens))\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [[t.lower() for t in token] for token in tokens]\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only_list = [[t for t in lower_token if t.isalpha()] for lower_token in lower_tokens]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [[t for t in alpha_only if t not in english_stops] for alpha_only in alpha_only_list]\n",
    "\n",
    "len_array = [len(token_array) for token_array in no_stops]\n",
    "# Use reduce() to apply a lambda function over stark: result\n",
    "total_tokens = reduce(lambda item1, item2: item1+item2, len_array)\n",
    "print(\"Total of words after removing stop words: {}\".format(total_tokens))\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "articles_lemmatized = [[wordnet_lemmatizer.lemmatize(t, get_wordnet_pos(t)) for t in no_stop if nltk.pos_tag([t])[0][1][0].upper()=='N' or nltk.pos_tag([t])[0][1][0].upper()=='V'] for no_stop in no_stops]\n",
    "\n",
    "len_array = [len(article) for article in articles_lemmatized]\n",
    "# Use reduce() to apply a lambda function over stark: result\n",
    "total_tokens = reduce(lambda item1, item2: item1+item2, len_array)\n",
    "print(\"Total of words after removing words except noun: {}\".format(total_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Remove organization, location nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(new_cases.shape[0]):\n",
    "    # Create a new document: doc\n",
    "    doc = nlp(new_cases.text[i])\n",
    "\n",
    "    # Print all of the found entities and their labels\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_=='ORG' or ent.label_=='GPE' or ent.label_=='LOC':\n",
    "            words = ent.text.split()\n",
    "            for word in words:\n",
    "                word = word.lower()\n",
    "                while word in articles_lemmatized[i]:\n",
    "                    articles_lemmatized[i].remove(word) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of words after removing words in ORG, GPE, LOC: 415776\n"
     ]
    }
   ],
   "source": [
    "len_array = [len(article) for article in articles_lemmatized]\n",
    "# Use reduce() to apply a lambda function over stark: result\n",
    "total_tokens = reduce(lambda item1, item2: item1+item2, len_array)\n",
    "print(\"Total of words after removing words in ORG, GPE, LOC: {}\".format(total_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Predict industry type with cleaned texts by using similarity method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert list into string for each text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_string = [\" \".join(article) for article in articles_lemmatized]\n",
    "key_list = list(industry_words.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create nlp object for each industry category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_industry_list = []\n",
    "for key, value in industry_words.items():\n",
    "        doc_industry_list.append(nlp(industry_words[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example : 16th text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s how AI can assist medical science in telling a patient’s lifespan\n",
      "0.502665953962633\n",
      "0.560985686202927\n",
      "0.6732316071532943\n",
      "0.6527046354124848\n",
      "0.6223831246228051\n",
      "0.5929365454180491\n",
      "0.7317095407518914\n",
      "0.6686644554870524\n",
      "0.7700925818314376\n",
      "0.6989430444817823\n",
      "0.6479598292617698\n",
      "0.6451362461295347\n",
      "Public and Social sector\n"
     ]
    }
   ],
   "source": [
    "# Create nlp object for an cleaned article\n",
    "doc_article = nlp(articles_string[16])\n",
    "print(new_cases.title[16])\n",
    "similarities = []\n",
    "for doc_industry in doc_industry_list:\n",
    "    similarity = doc_article.similarity(doc_industry)\n",
    "    print(similarity)\n",
    "    similarities.append(similarity)\n",
    "max_value = max(similarities)\n",
    "max_position = similarities.index(max_value)\n",
    "industry_type = key_list[max_position]\n",
    "print(industry_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict industry type with similarity method along two nlp object for all texts in industry_type_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_type_list = []\n",
    "for article in articles_string:\n",
    "    # Create nlp object for an cleaned article\n",
    "    doc_article = nlp(article)\n",
    "    similarities = []\n",
    "    for doc_industry in doc_industry_list:\n",
    "        similarities.append(doc_article.similarity(doc_industry))\n",
    "    max_value = max(similarities)\n",
    "    max_position = similarities.index(max_value)\n",
    "    industry_type = key_list[max_position]\n",
    "    industry_type_list.append(industry_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Predict industry type with tf-idf words by using similarity method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1626\n"
     ]
    }
   ],
   "source": [
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(articles_lemmatized)\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles_lemmatized]\n",
    "\n",
    "# How much text in corpus_sorted\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tf-idf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "tfidf_weights = [sorted(tfidf[doc], key=lambda w: w[1], reverse=True) for doc in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create nlp object for each industry category and key_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = list(industry_words.keys())\n",
    "doc_industry_list = []\n",
    "for key, value in industry_words.items():\n",
    "        doc_industry_list.append(nlp(industry_words[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example : 16th text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lifespan adelaide surpasses disease ai prediction patient survey congestive emphysema expo job absence chronic organ image resident illness predict analysis'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_string_single = [dictionary.get(term_id) for term_id, weight in tfidf_weights[16][:20]]\n",
    "article_string_single = \" \".join(article_string_single)\n",
    "article_string_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3804882291089601\n",
      "0.4508796499406264\n",
      "0.5061033561902708\n",
      "0.531079364987485\n",
      "0.5476590285682619\n",
      "0.46029947173572866\n",
      "0.7917154485345823\n",
      "0.7025677445623351\n",
      "0.6184250942701971\n",
      "0.5402324152260866\n",
      "0.48449173756379493\n",
      "0.5036701114194474\n",
      "Health Care\n"
     ]
    }
   ],
   "source": [
    "doc_article = nlp(article_string_single)\n",
    "similarities = []\n",
    "for doc_industry in doc_industry_list:\n",
    "    similarity = doc_article.similarity(doc_industry)\n",
    "    print(similarity)\n",
    "    similarities.append(similarity)\n",
    "max_value = max(similarities)\n",
    "max_position = similarities.index(max_value)\n",
    "industry_type = key_list[max_position]\n",
    "print(industry_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict industry type with similarity method along two nlp object for all texts in industry_type_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select the most important first 20 words for each text a and convert list into string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_strings = [[dictionary.get(term_id) for term_id, weight in tfidf_weight[:20]] for tfidf_weight in tfidf_weights]\n",
    "articles_strings = [\" \".join(articles_string) for articles_string in articles_strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_type_list = []\n",
    "for article in articles_string:\n",
    "    # Create nlp object for an cleaned article\n",
    "    doc_article = nlp(article)\n",
    "    similarities = []\n",
    "    for doc_industry in doc_industry_list:\n",
    "        similarities.append(doc_article.similarity(doc_industry))\n",
    "    max_value = max(similarities)\n",
    "    max_position = similarities.index(max_value)\n",
    "    industry_type = key_list[max_position]\n",
    "    industry_type_list.append(industry_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1626"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(industry_type_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
